---
title: "Reciclaje"
author: "CruzJulian"
date: "15 de abril de 2017"
output: html_document
---


## Discusión


Durante la aplicacion de los métodos se evidenciaron algunas dificultades respecto al algoritmo EP-Means. El costo computacional del mismo hizo imposible el trabajo con el conjunto de datos completo, en consecuencia fue necesario tomar una muestra aleatoria de los datos. La poca eficiencia del algoritmo puede deberse a distintas causas, entre ellas el constante cálculo de la distancia entre funciones empíricas, que implica integración numérica. 

El proceso de detección de diferencias estadísticamente significativas en los genes no presenta una exactitud deseable. No resulta acertado suponer que el **##** porciento de los genes son diferencialmente expresados. Esto se debe al tamaño de los clusters, ya que cada cluster reune cientos de datos, lo cual aumenta la probabilidad de rechazo.

# Conclusiones

- El uso del algoritmo EP-Means es computacionalmente más costoso que SAM y acde.

- El algoritmo EP-Means detecta expresión genética en el 83% de los genes, lo cual no es acertado. SAM y acde resultan bastante más conservadores y por ende más convincentes.

# texto guía

El Análisis de Componentes Principales es un método no supervisado que busca resumir un conjunto de variables en ejes factoriales mediante una transformación ortogonal. Debido a que estas transformaciones ortogonales son isomorfismos, el número de ejes factoriales posible corresponde al número de variables incluidas en el conjunto de interés.

Los ejes factoriales reúnen el total de la variabilidad del conjunto de datos de forma descendente, así el primero recoge la mayor varianza posible, el segundo reúne una variabilidad menor que el primero pero contiene la mayor varianza restante y así sucesivamente. Estos ejes son calculados bajo la restricción de ortogonalidad, obteniendo un conjunto de ejes ortogonales. La cantidad de ejes factoriales a conservar depende de la proporción de variabilidad que el investigador necesite recoger.

Este análisis tiene tres usos comunes en la literatura; los ejes factoriales recogen de manera descendente la varianza de los datos, por consiguiente es posible tomar un número reducido de ejes cubriendo un porcentaje importante de la información presente en los datos, disminuyendo la dimensión de los mismos. En este mismo sentido el uso de los ejes factoriales para interpretación gráfica de los datos proporciona visualizaciones óptimas e intuitivas. Por último, si las variables presentan correlaciones fuertes, es posible dar interpretación a los ejes factoriales y crear indicadores que midan aspectos específicos que no tienen una expresión explícita en la base de datos. [@Jolliffe2002]


El algoritmo K-Means [@HARTIGAN] es teóricamente equivalente a la estimación vía EM de una mixtura compuesta por $K$ distribuciones normales homocedásticas.

En este sentido el algoritmo tiene dos partes. Un paso de estimación, que define el centroide de cada grupo como el promedio de los individuos pertenecientes al mismo y un paso de maximización, que reasigna los individuos al grupo más cercano usando su distancia a los distintos centroides. Estos dos pasos se repiten hasta lograr convergencia.

El uso de distancias puede verse afectado por distintas causas, entre ellos el factor de escala de las variables; en el sentido que las variables de mayor escala tienen un mayor peso en el cálculo de las distancias entre individuos. En consecuencia es necesario un tratamiento previo de normalización o estandarización de las mismas. La solución mostrada en Lebart [@LEBART] es tomar como insumo el resultado del ACP, que además de eliminar el ruido inherente a los datos produce ejes de escalas similares provenientes de las variables estandarizadas.

Asimismo es necesario atender detalles como criterios de parada para la convergencia, distancia a usar (euclidiana, manhattan, Crámer-von Mises), el algoritmo EP-Means hace uso de la distancia Earth mover's, sin embargo es necesario realizar pruebas con varias opciones. Una definición correcta de los valores iniciales puede facilitar la convergencia, Lebart propone como valores iniciales los centroides provenientes del método de Ward.

El teorema de Glivenko Cantelli es el eje teórico del algoritmo. Establece la convergencia de la función empírica de densidad acumulada a la función de densidad acumulada teórica.

Sea $\{X\}_i, i = 1...m$ una muestra aleatoria con función de distribución acumulada $F$. La función de distribución empírica para el conjunto de v.a se define por:

\[F_n(x) = \frac{1}{n}\sum\limits_{i=1}{m} I_{(-\infty, x]} (X_i) \]

Donde $I_A$ es la función indicadora en el conjunto $A$. Para cada $x$ fijo $F_n(x)$ es una secuencia de variables aleatorias las cuales convergen de manera casi segura a  $F(x)$.

El supuesto de independencia no es necesario para este teorema, y esto es fácilmente se observa que la misma conclusión se da cuando la secuencia de variables aleatorias es estrictamente una secuencia estacionaria y ergódica [@GCTHEO].

Con esto en mente agrupar las funciones de densidad empíricas por su cercanía puede mostrar los comportamientos asintóticos de las nubes de puntos.




# Introducción


El proceso de expresión génica consiste en la manifestación de un gen o grupo de genes en sus procesos de transcripción y traducción; a su vez, el análisis de la expresión génica se define como la determinación de patrones de genes expresados a nivel de la transcripción bajo circunstancias específicas o en células específicas [@Herraez2012]. Para el estudio de la expresión génica existen dos tecnologías: la secuenciación del RNA y el análisis de microarreglos, siendo este último el que presenta en la actualidad una cantidad mayor de propuestas de análisis debido a que se trata de tecnología pionera en secuenciación del RNA (RNAseq) [@Zhao2014].

Dadas las caracteristicas de los datos que se obtienen en los experimentos de microarreglos se pueden presentar datos con ruido y outliers. Además la cercanía entre los genes pueden interferir en la identificación de los mismos y por ello representan un desafío a nivel estadistico y computacional al momento de aplicar metodologías para identificar genes expresados diferencialmente [@Jiang2004]. 

Adicionalmente es posible aplicar metodologías de clustering que permitan entender las funciones de los genes, la regulación génica y los procesos celulares, además los genes que presenten patrones similares de expresión pueden ser agrupados junto con las funciones celulares que estos desempeñen [@Jiang2004]. 

Así pues el propósito de este capítulo es abordar el problema de la expresión diferencial mediante el método J-nubes, comparando los resultados obtenidos con el método SAM (Significance Analysis of Microarrays) [@Chu2011] y haciendo uso de acde (Artificial Components Detection of Differentially Expressed Genes) [@Acosta2015] como visualización. Primero se explora el tema de clustering en datos genómicos, desarrollado en gran parte usando microarrays; luego se presentan los métodos, aclarando en detalle el ejercicio; una breve sección de resultados conduce a un análisis sucinto de los mismos.

# Antecedentes

## Microarreglos 

El análisis de microarreglos es una técnica de biología molecular desarrollada en los años 90 que ha estado enfocada en el análisis de colecciones de datos genéticos y se ha utilizado para monitorear la expresión génica de miles de genes de forma paralela, utilizando matrices para análisis miles de transcritos génicos y dando la posibilidad de responder a una amplia gama de problemas biológicos, como la identificación de genes expresados diferencialmente entre tejidos enfermos y sanos, a evolución de la regulación génica y la respuesta a fármacos entre otros. Los experimentos aplicados con mayor frecuencia son microarreglos con cDNA (Copia de ADN) y los microarreglos de nucleótidos conocidos como oligochip [@Jiang2004][@Zhao2014].

La técnica pretende medir el nivel de expresión del ADN por medio de la relación entre la señal de prueba de la muestra y la muestra control, que han sido previamente marcadas con un fluorocromo. Tanto la muestra como el control son marcados con fluorocromos y montados en un chip leído a través de una plataforma de microarreglos [@Jiang2004].

Generalmente los experimentos de microarreglos cuentan con datos de 1000 a 10000 genes (pueden ser más, ya que los chips pueden ser diseñados) pero con un número de muestras usualmente menores a 100 [@Jiang2004].

Entre las limitaciones de estas técnicas está el background de la hibridización, que limita la exactitud de las mediciones de expresión particularmente a los transcritos de baja frecuencia, los datos muchas veces son susceptibles al ruido dado que las mediciones dependen de la emisión de luz de las sondas marcadas y pueden generar de por sí falsos positivos. La identificación clara de outliers que son datos  obtenidos por respuesta experimental de los organismos a las condiciones expuestas se ve limitada por el ruido experimental de los datos [@Jiang2004][@Zhao2014]. 

## Clustering en datos genéticos

Existen tres modelos de clustering que han sido aplicados para el análisis de datos de microarrelgos, tales modelos como son [@Jiang2004]: 

1. Clustering basado en genes, donde los genes son tratados como objetos mientras que las muestras (o condiciones) son tratadas como características y cada grupo corresponde a un fenotipo. 

2. Clustering basado en la muestra: Las muestras son tratadas como objetos y los genes como características.

3. Subespacio de clustering: Donde se toman los clusters formados por un subset de genes de una muestra, esto dado al principio biológico de que solo un grupo pequeño de genes participan en cualquier proceso celular de interés y se lleva a cabo por un subconjunto de muestras.

## Medidas de proximidad.

Entre las medidas de proximidad utilizadas para este tipo de análisis se encuentran: la distancia eclidiana, que presenta el inconveniente de que no puntúa bien los datos escalonados de expresión génica. El coeficiente de Pearson es el la medida más popular para el análisis de este tipo de datos pues ofrece una medición efectiva para los datos de expresión génica pero no es robusto con los outliers y puede generar falsos positivos cuando se asigna un puntaje de alta similariadad. Otra media es la correlación de Jackknife que elimina los outliers evitando la “dominación” pero implica la enumeración de diferentes combinaciones de características para luego eliminarlas, lo que hace que computacionalmente sea costo.  Finalmente esta la correlación de Sperman ya que no requiere una distribución Gaussiana y es muy robusta para trabajar los outliers pero presenta el inconveniente de la perdida de datos [@Jiang2004].   

## Desafíos del clustering de genes. 

La utilización de tecnicas de clustering para el analisis de expresión diferencial en datos obtenidos a partir de microarreglos son [@Jiang2004]:

1. Los algoritmos a utilizar deben depender lo menos posible del conocimiento previo, para poder visualizar las estructuras de los datos y poder visualizar la distribución de los mismos.

2. Los datos de microarreglos contienen mucho ruido, por lo tanto los algoritmos deben ser capaces de extraer información útil en esa condición.

3. Los datos están altamente conectados entre sí,  por lo tanto los algoritmos deben ser capaces de manejar esta situación.

4. A nivel biológico podría no ser de interés los clusters de los genes, pero sí la relación entre los clusters y la relación de los genes dentro del mismo clusters, por lo tanto los algoritmos deben ser capaces de particionar los datos sino que también de mostrar una estructura gráfica del cluster.

