# Introducción

El desarrollo tecnológico de los últimos 20 años ha cambiado por completo los conceptos y procesos de análisis de la información. El aumento de la capacidad computacional conlleva la aparición de algoritmos y técnicas que, exigiendo mucho más sobre la máquina, permiten extraer información más precisa y útil. De manera simultánea estos desarrollos han conducido a la gestión y almacenamiento de grandes conjuntos de datos; conjuntos cuyo procesamiento y análisis presentan retos en términos estadísticos. Esto ha impulsado el estudio y desarrollo de diversas metodogías y herramientas analíticas, entre ellas se encuentra la Función de Densidad Acumulada Empírica, cuyo estudio es el eje central del presente trabajo.

Dada una muestra aleatoria $X_i$, la Función Empírica de Densidad Acumulada se define de la siguiente manera:

\[\overwide{f}(x) = \frac{1}{n} \sum_{i = 1}^n I_{(-\infty, x]}(X_i)\]

Se trata de una función escalonada que aumenta en $\frac{1}{n}$ en cada uno de los puntos de la muestra. El teorema de Glivenko Cantelli (ver más adelante) garantiza que esta función converge puntualmente con probabilidad 1 a la función de densidad acumulada de $X$. En consecuencia las función empírica de densidad acumulada es un estimador de la función de densidad acumulada asociada a $X$. El tamaño de los escalones es $\frac{1}{n}$, sin embargo es posible reemplazarlo por los pesos muestrales $a_i$ desde que todos sean positivos y su suma sea igual a 1.

En este contexto un conjunto de puntos es representado completamente por una curva. En consecuencia es posible representar una colección de conjuntos de puntos usando una colección de curvas. (el autor de epmeans) aplica algoritmos de agrupación a estas curvas, separándolas en un número dado de grupos de curvas similares entre sí. De esta manera es posible afirmar que EPMeans es un algoritmo de agrupación asintótica no paramétrico aplicable en grandes conjuntos de datos y tiene por objetivo encontrar una estructura de agrupación basada en la forma distribucional empírica dada por los datos. Este trabajo tiene aplicación en varios problemas estadísticos  que se mostrarán durante el desarrollo del estudio, uno de estos es la comparación de muestras de varias distribuciones.

@Aponte2012 aborda en su disertación de maestría el problema de comparación de muestras de varias distribuciones. Dado un número determinado de muestras, el problema consiste en encontrar diferencias distribucionales entre ellas. En respuesta a este problema existen pruebas estadísticas como ANOVA [@Montgomery2004], Kruskall Wallis [@KWTEST], Friedman [@Friedman1937], amén de un largo número de procedimientos de comparaciones mútiples [@CARSWA].

Las herramientas actuales hacen posible establecer cuándo una distribución presenta valores significativamente distintos a otra, sin embargo poseen inconvenientes en cuanto a interpretabilidad y visualización. La construcción de modelos estadísticos de tipo inferencial tiene en general supuestos distribucionales que en muchas ocasiones no permiten una conceptualización nítida del comportamiento global de los datos. Un resultado usual en este ambito es $\mu_i = \mu_j$, $\mu_j = \mu_k$ y $\mu_i \noteq \mu_k$. Esto sucede porque el error tipo I aumenta al aumentar el número de pruebas; en respuesta se amplían los intervalos de confianza llegando así a colnclusiones contraintuitivas.

Henderson y Clark (cita) distinguen dos componentes en la creación de conocimiento. El conocimiento modular se refiere a las partes del proceso y el conocimiento arquitectural se refiere a la estructura del mismo. Agrupar las poblaciones a partir de su comportamiento estocástico facilita la creación de perfiles, que muestren las tendencias principales con una visualización específica. Es posible decir que se trata de un avance arquitectural que nace en respuesta a las necesidades del investigador.

El presente trabajo recoge una fundamentación matemática al algoritmo EP-Means [@EPMEANS] introduciendo definiciones formales de las nubes de puntos, y de las funciones empíricas; y estudiando sus distancias y sus propiedades. Si bien la idea original del algoritmo se plantea a nivel univariado, la formalización permite generalizar sus conceptos al ámbito multivariado. Con esto se obtiene el soporte teórico necesario antes de entrar en el contexto aplicado.

**El algoritmo EP-Means hace uso de K-Means [@HARTIGAN] para agrupar las curvas. Una de las pre Implementar de el método de Ward [@WARD] y los algoritmos de agrupación ECSAGO [@Leon2010] y Chamaleon [@Karypis1999] y examinar l.**

**Con estas bases se obtiene una propuesta metodológica dirigida al problema de comparación no paramétrica de varias poblaciones, ya mencionado. Esta propuesta se evalua desde el punto de vista experimental mediante un estudio de Monte Carlo en dos etapas. La primera consiste en comparar el uso de distintas herramientas y así observar cuales son los parámetros que optimizan el desempeño de la propuesta. La segunda compara otros enfoques y muestra el desempeño de la propuesta en relación con otras técnicas estadísticas diseñadas con objetivos similares.**

**El estudio finaliza con la aplicación de la metodología propuesta en datos reales. Para este fin se dispone de los datos correspondientes a las pruebas Saber11, facilitados para la investigación por el Instituto Colombiano para la Evaluación de la Educación ICFES, y los datos “phytophthora”, de expresión genética presentes en el paquete “acde” de R [@Acosta2015].**
