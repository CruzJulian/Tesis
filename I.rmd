---
title: "Ensayo"
author: "Julian Cruz"
output: 
  pdf_document:
    keep_tex: true
    toc: false
    number_sections: true
bibliography: Biblio.bib
csl: apa.csl

---

# Introducción

 - Intro
  - Procesos que renuevan la estadística
   - raise of the mecanism of recolecion de datos
  - De dónde viene?
   - Problema de investigación
   - alterntivas tradicionales
   - EPMeans
 - Obj
  - Gen
  - Esp
 
El desarrollo tecnológico de los últimos 20 años ha cambiado por completo los conceptos y procesos de análisis de la información. Las cifras son indimensionables: "En 2007, la humanidad pudo almacenar 2,9 × $10^{20}$ bytes óptimamente comprimidos, comunicar casi 2 × $10^{21}$ bytes y llevar a cabo $6.4 × 10^18$ instrucciones por segundo en equipos de uso general."[1] El auge y la consolidación de mecanismos cada vez más sofisticados de recolección de datos exigen la constante creación y actualización de las técnicas estadísticas.

De manera simultánea, la capacidad computacional crece rápidamente. La ley de Moore, vigente desde su formulación en 1965, expresa que aproximadamente cada dos años se duplica el número de transistores en un microprocesador [2]. Esto conlleva la aparición de algoritmos y técnicas que, exigiendo mucho más sobre la máquina, permiten extraer información más precisa y útil. Estos avances han conducido a la gestión y almacenamiento de grandes conjuntos de datos; conjuntos cuyo procesamiento y análisis requiere desarrollos teóricos diseñados para este tipo de problemas.

El presente trabajo nace como respuesta a un problema específico de este tipo. En el marco de la tesis doctoral [3] el diagnóstico del problema de diseño requería clasificar 60 variedade de rosa de acuerdo a su resistencia al corte medida usando la fuerza en lbf necsaria para cortar los tallos. Los datos están conformados por dos variables: La resistencia al corte, variable continua y la variedad, categórica con 60 categorías; donde cada registro corresponde a un tallo de rosa cortado. Un problema similar, en política pública, consiste en realizar una clasificación de los municipios de Colombia con base en los puntajes individuales de las pruebas Saber 11, del ICFES. En este caso los datos están conformados por dos variables: el puntaje en la prueba Saber 11, variable coontinua y el municipio, variable categórica con 1122 categorias, según el DANE; donde cada registro corresponde a un estudiante que presentó la prueba. En ambos casos se trata de dos variables, el problema consiste en agrupar las categorías de la variable nominal con base en similaridades de sus valores alcanzados en la variable continua.

El caso bivariado cuanti-cualitativo, ha sido estudiado por varios autores, creando técnicas estadísticas propicias para responder interrogantes específicos. Entre ellas ANOVA [@Montgomery2004], Kruskal - Wallis [@KWTEST], Friedman [@Friedman1937], amén de un largo número de procedimientos de comparaciones mútiples [@CARSWA]. Estas herramientas hacen posible establecer cuándo las distribuciones asociadas a las distintas categoorías presentan valores significativamente distintoentre ellas. La popularidad de estos métodos radica en que resuelven de manera eficiente el problema para el que fueron diseñados. La eficiencia, en este contexto, se refiere al uso de la menor cantidad de datos posible. Una de las críticas más frecuentes  consiste en que al comparar las medias condicionales un resultado usual es $\mu_i = \mu_j$, $\mu_j = \mu_k$ y $\mu_i \neq \mu_k$. Esto sucede porque el error tipo I aumenta al aumentar el número de pruebas; en respuesta se amplían los intervalos de confianza llegando así a conclusiones que, como esta, resultan contraintuitivas.

Al intentar resolver problemas con muchas categorías como el mostrado, estas técnicas poseen inconvenientes en cuanto a interpretabilidad y visualización. La construcción de modelos estadísticos de tipo inferencial tiene, en general, supuestos distribucionales que en muchas ocasiones no permiten una conceptualización nítida del comportamiento global de los datos. Adicionalmente, el problema requiere una clasificación de las categorías; establecer diferencias estadísticamente significativas no es suficiente.

Esto ha impulsado el estudio y desarrollo de diversas metodogías y herramientas analíticas, entre ellas se encuentra la Función de Densidad Acumulada Empírica (FDAE), cuyo estudio es el eje central del presente trabajo. En este contexto un conjunto de puntos es representado completamente por una curva. En consecuencia es posible representar una colección de conjuntos de puntos usando una colección de curvas. La técnica EP-Means [@EPMEANS] aplica algoritmos de agrupación a estas curvas, separándolas en un número dado de grupos de curvas similares entre sí. EP-Means hace uso de K-Means [@HARTIGAN] para agrupar las curvas. Uno de los retos del presente estudio es investigar su desempeño cambiando el algoritmo de agrupación. Resulta pertinente comparar resultados usando los algoritmos de agrupación de Ward [@WARD], ECSAGO [@Leon2010] y DBSCAN [@DBSCAN]. De esta manera es posible afirmar que EP-Means es una técnica de agrupación asintótica, no paramétrica, aplicable en grandes conjuntos de datos, cuyo objetivo es encontrar una estructura de agrupación basada en la forma distribucional empírica dada por los datos.

El presente trabajo estructura una fundamentación matemática del problema de investigación introduciendo definiciones formales de las nubes de puntos, y de las funciones empíricas; y estudiando sus distancias y sus propiedades. Si bien el problema está planteado a nivel univariado, la formalización permite generalizar sus conceptos al ámbito multivariado. Con esto se obtiene el soporte teórico necesario antes de entrar en el contexto aplicado.

Una vez establecidas las bases es necesario realizar alguna aplicación para apreciar su desempeño. Esta propuesta se evalua mediante un estudio de Monte Carlo en dos etapas. La primera consiste en comparar el uso de distintos algoritmos de agrupación y así observar cuales resultan más apropiados. La segunda muestra el desempeño de la propuesta en relación con otras técnicas estadísticas diseñadas con objetivos similares: LSD de Fisher y Test de Duncan.

El estudio finaliza con la aplicación de la metodología propuesta en datos reales. Para este fin se dispone de los datos correspondientes a las pruebas Saber11, facilitados para la investigación por el Instituto Colombiano para la Evaluación de la Educación ICFES, y los datos “phytophthora”, de expresión genética presentes en el paquete “acde” de R [@Acosta2015].

# Objetivos

## General

Analizar, fundamentar e implementar el algoritmo EP-Means de agrupación estocástica.

## Específicos

- Formalizar matemáticamente la definición de nubes de puntos y estudiar sus propiedades (dimensión, orden, unión, suma y equivalencia).

- Analizar el algoritmo EP-Means a la luz de estas propiedades. Explicar la estructura estocástica de las nubes de puntos usando métodos para visualización y reducción de dimensiones (ACP) y agrupación (e.g. K-Means, Ward, Chamaleon, ECSAGO, entre otros). 

- Evaluar la propuesta resultante mediante un estudio de simulación comparando distintos enfoques.

- Implementar y aplicar la propuesta resultante en datos reales.

- Crear un paquete en R.

# Antecedentes

## Algoritmo EP-Means

EP-Means [@EPMEANS] es una nueva técnica que aplica el algoritmo K-Means sobre funciones empíricas de distribución acumulada. Para esto hace uso de la distancia Earth Mover's [@emd] entre estas funciones. El desarrollo desemboca en un algoritmo eficiente, empírico, no paramétrico y basado en distancias.

Sean $\textbf{X}=\{X_l\}_{l\leq n}$ y $\textbf{Y}=\{Y_l\}_{l\leq n}$ muestras aleatorias de tipo continuo y categórico respectivamente, siendo $a_1 , ..., a_k$ los valores posibles (tratamientos) de $Y_l$. El objetivo de EP-Means es clasificar los niveles $a_i$ en clases intrahomogéneas respecto a $\textbf{X}$. Cada nivel $a_i$ tiene $n_i$ representantes dentro de la muestra, de manera que para cada nivel es posible calcular una FEDA correspondiente. Otra forma de verlo es pensar que la variable categórica $\textbf{Y}$ divide la muestra en submuestras y que cada una de estas tiene asociada una FEDA.

El teorema de Glivenko-Cantelli [@GCTHEO] asegura la convergencia de la FEDA a la Función de Densidad Acumulada Teórica cuando la muestra crece. En consecuencia es intuitivo pensar que las submuestras asociadas a funciones FEDA similares provienen de la misma distribución. Aparece entonces el concepto de similaridad entre funciones; la técnica EP-Means opta por la distancia Earth Mover's [@emd].

La distancia Earth Mover's entre funciones está dada por la fórmula

$$ EMD(P,Q) = \int\limits_{x = 0}^1 | CDF_P^{-1}(x) - CDF_Q^{-1}(x) | dx $$

cuyas propiedades de distancia resultan, si no obvias, al menos intuitivas. Usando esta distancia es posible aplicar el algoritmo K-Means al conjunto de funciones FEDA logrando grupos de funciones similares. Por consiguiente se tiene una división de los niveles $a_i$ de la variable $\textbf{Y}$ en grupos cuyos valores en $\textbf{X}$ rienen comportamientos distribucionales parecidos.

Una idea bastante parecida fue presentada por @Barrera2014, en esta propuesta se suavizan las distribuciones acumuladas estimadas e implementan herramientas diseñadas para datos funcionales. El artículo propone la implementación de un método de agrupamiento jerárquico para funciones de densidad considerándolas datos funcionales. Para la implementación se representan en forma discreta de las funciones de densidad de probabilidad, posteriormente se usa la distancia de Hellinger con el fin de medir las distancias entre todas las curvas, y a su vez, se construye una estructura de agrupamiento jerárquico.

La idea de agrupar los datos a partir de su distribución es planteada por @Applegate2011, que aplica K-Means sobre histogramas multivariadoss. No obstante el número de clases de los histogramas pueden influir sobre los agrupamientos. El uso de funciones empíricas resuelve esto, sin embargo en @EPMEANS se desarrolla únicamente el caso univariado, dejando el multivariado para trabajo futuro.

## Teorema de Glivenko Cantelli

El teorema de Glivenko Cantelli es el eje teórico del algoritmo. Establece la convergencia de la función empírica de densidad acumulada a la función de densidad acumulada teórica.

Sea $\{X\}_i, i = 1...m$ una muestra aleatoria con función de distribución acumulada $F$. La función de distribución empírica para el conjunto de v.a se define por:

\[F_n(x) = \frac{1}{n}\sum\limits_{i=1}^m I_{(-\infty, x]} (X_i) \]

Donde $I_A$ es la función indicadora en el conjunto $A$. Para cada $x$ fijo $F_n(x)$ es una secuencia de variables aleatorias las cuales convergen de manera casi segura a  $F(x)$.

El supuesto de independencia no es necesario para este teorema, y esto es fácilmente se observa que la misma conclusión se da cuando la secuencia de variables aleatorias es estrictamente una secuencia estacionaria y ergódica [@GCTHEO].

Con esto en mente agrupar las funciones de densidad empíricas por su cercanía puede mostrar los comportamientos asintóticos de las nubes de puntos.

### Método de clasificación: K-means

Como se dijo antes el algoritmo K-Means [@HARTIGAN] es teóricamente equivalente a la estimación vía EM de una mixtura compuesta por $K$ distribuciones normales homocedásticas.

En este sentido el algoritmo tiene dos partes. Un paso de estimación, que define el centroide de cada grupo como el promedio de los individuos pertenecientes al mismo y un paso de maximización, que reasigna los individuos al grupo más cercano usando su distancia a los distintos centroides. Estos dos pasos se repiten hasta lograr convergencia.

El uso de distancias puede verse afectado por distintas causas, entre ellos el factor de escala de las variables; en el sentido que las variables de mayor escala tienen un mayor peso en el cálculo de las distancias entre individuos. En consecuencia es necesario un tratamiento previo de normalización o estandarización de las mismas. La solución mostrada en Lebart [@LEBART] es tomar como insumo el resultado del ACP, que además de eliminar el ruido inherente a los datos produce ejes de escalas similares provenientes de las variables estandarizadas.

Asimismo es necesario atender detalles como criterios de parada para la convergencia, distancia a usar (euclidiana, manhattan, Crámer-von Mises), el algoritmo EP-Means hace uso de la distancia Earth mover's, sin embargo es necesario realizar pruebas con varias opciones. Una definición correcta de los valores iniciales puede facilitar la convergencia, Lebart propone como valores iniciales los centroides provenientes del método de Ward.

## Análisis de Componentes Principales

El Análisis de Componentes Principales es un método no supervisado que busca resumir un conjunto de variables en ejes factoriales mediante una transformación ortogonal. Debido a que estas transformaciones ortogonales son isomorfismos, generalmente el número de ejes factoriales posible corresponde al número de variables incluidas en el conjunto de interés.

Los ejes factoriales reúnen el total de la variabilidad del conjunto de datos de forma descendente, así el primero recoge la mayor varianza posible, el segundo reúne una variabilidad menor que el primero pero contiene la mayor varianza restante y así sucesivamente. Estos ejes son calculados bajo la restricción de ortogonalidad, obteniendo un conjunto de ejes ortogonales. La cantidad de ejes factoriales a conservar depende de la proporción de variabilidad que el investigador necesite recoger.

Este análisis tiene tres usos comunes en la literatura; los ejes factoriales recogen de manera descendente la varianza de los datos, por consiguiente es posible tomar un número reducido de ejes cubriendo un porcentaje importante de la información presente en los datos, disminuyendo la dimensión de los mismos. En este mismo sentido el uso de los ejes factoriales para interpretación gráfica de los datos proporciona visualizaciones óptimas e intuitivas. Por último, si las variables presentan correlaciones fuertes, es posible dar interpretación a los ejes factoriales y crear indicadores que midan aspectos específicos que no tienen una expresión explícita en la base de datos. [@Jolliffe2002]

## Plan de acción

El estudio está proyectado en dos fases. La primera fase es teórica, se trata de observar los alcances teóricos de la propuesta y establecer sus características principales. La segunda fase es aplicada, se establecen directrices para aplicación, se implementa y se evalúa de acuerdo a criterios establecidos.

### Fundamentación y generalizaciión

En el modelamiento de grandes conjuntos de datos se toma cada registro como un punto en un espacio multidimensional. En un conjunto de datos usual todos los puntos presentan la misma importancia, sin embargo existen muchos casos en donde el diseño muestral da a cada punto un peso distinto dentro de la muestra. Formalizar el concepto de nubes de puntos tomando en cuenta los registros y la importancia (peso muestral) de cada uno es un primer paso hacia una tratamiento más general de la información.
El estudio teórico de las propiedades matemáticas de las nubes de puntos ya formalizadas permite obtener definiciones formales de distancia, unión, equivalencia, dimensión y orden. Creando una estructura consistente y robusta, que permite encontrar comportamientos, inconvenientes y similaridades que posteriormente pueden influir en la construcción de una metodología aplicada.

### Aplicación

A partir de las nociones de orden, dimensión, equivalencia, distancia y unión entre nubes de puntos, se configura un escenario finitodimensional donde resultan propicias las técnicas para analizar la estructura estocástica y generar agrupaciones (e.g. Ward, K-Means, ECSAGO, entre otros). Cada una de ellas con una motivación específica dentro del estudio. La técnica de ACP describe visualmente la colección, mientras Ward, K-Means, y ECSAGO generan clasificaciones, que al ser estocásticas presentan propiedades inferenciales deseadas y comportamiento robusto frente al ruido y a los atípicos.
El proceso de validación interna se realiza mediante la prueba Kruskall-Wallis y validación cruzada generalizada. El proceso de validación externa se realiza comparando la metodología propuesta con otros métodos que abordan problemas similares vía simulación. La aplicación con datos reales se da en dos direcciones, en política pública se propone una estratificación de municipios a partir de los datos de la prueba Saber 11 del ICFES, y en datos transcriptómicos se propone un método de detección de expresión diferencial utilizando los datos de la librería acde de R.

### Política de datos

El proyecto de investigación hace uso de los datos de los puntajes del examen Saber 11 que el Instituto Colombiano para la Evaluación de la Educación (ICFES) tiene con acceso público. Así mismo la aplicación en datos transcriptómicos se realiza usando datos de la librería acde de R, que son accesibles al público.
Todo el trabajo de investigación es abierto, reproducible y accesible a través de la plataforma GitHub. La licencia sobre los resultados del estudio, si aplica, es pública tanto para el resultado teórico - metodológico como para el código, toda vez que se busca generar un paquete de R a partir de este último.

### Cronograma

```{r, echo = FALSE}
library(knitr)
data.frame(
  Actividad = c("Revisión bibliográfica", "Escritura del documento", "Fundamentación matemática", "Simulación", "Aplicación ICFES", "Aplicación Genómica", "Paquete R", "Discusión y conclusiones"),
  Ene = c(1, 1, 1, 0, 1, 0, 1, 0),
  Feb = c(1, 1, 1, 0, 1, 0, 1, 0),
  Mar = c(1, 1, 0, 1, 0, 1, 1, 0),
  Abr = c(1, 1, 0, 1, 0, 1, 1, 0),
  May = c(1, 1, 0, 0, 0, 0, 1, 1),
  Jun = c(1, 1, 0, 0, 0, 0, 1, 1)
) -> a
kable(a)
```

### Presupuesto

```{r, echo = FALSE}
data.frame(
  Recurso = c("Datos para aplicación", "Acompañamiento y dirección", "Equipos Informáticos",  "software", "Investigación"),
  Unidad = c("Número de bases de datos", "Horas de asesoría", "Número de equipos", "Licencias de software", "Horas de investigación"),
  Cantidad = c(2, 128, 1, 1, 384),
  Responsable = c("Estudiante", "Universidad Nacional de Colombia", "Estudiante", "Estudiante", "Estudiante")
) -> a
kable(a)
```


# Falta mucho


[1]

@article {Hilbert60,
	author = {Hilbert, Martin and L{\'o}pez, Priscila},
	title = {The World{\textquoteright}s Technological Capacity to Store, Communicate, and Compute Information},
	volume = {332},
	number = {6025},
	pages = {60--65},
	year = {2011},
	doi = {10.1126/science.1200970},
	publisher = {American Association for the Advancement of Science},
	abstract = {We estimated the world{\textquoteright}s technological capacity to store, communicate, and compute information, tracking 60 analog and digital technologies during the period from 1986 to 2007. In 2007, humankind was able to store 2.9 {\texttimes} 1020 optimally compressed bytes, communicate almost 2 {\texttimes} 1021 bytes, and carry out 6.4 {\texttimes} 1018 instructions per second on general-purpose computers. General-purpose computing capacity grew at an annual rate of 58\%. The world{\textquoteright}s capacity for bidirectional telecommunication grew at 28\% per year, closely followed by the increase in globally stored information (23\%). Humankind{\textquoteright}s capacity for unidirectional information diffusion through broadcasting channels has experienced comparatively modest annual growth (6\%). Telecommunication has been dominated by digital technologies since 1990 (99.9\% in digital format in 2007), and the majority of our technological memory has been in digital format since the early 2000s (94\% digital in 2007).},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/332/6025/60},
	eprint = {http://science.sciencemag.org/content/332/6025/60.full.pdf},
	journal = {Science}
}

[2]
@article{Moore65,
  added-at = {2011-05-23T17:50:46.000+0200},
  author = {Moore, Gordon E.},
  biburl = {http://www.bibsonomy.org/bibtex/2867f7a1c71867074780e958c5cec38e4/klik24},
  interhash = {886de6a8be647a56cc3de71d50149f5e},
  intrahash = {867f7a1c71867074780e958c5cec38e4},
  journal = {Electronics},
  keywords = {1965 Moore},
  month = {April},
  number = 8,
  timestamp = {2011-05-27T08:53:09.000+0200},
  title = {Cramming more components onto integrated circuits},
  volume = 38,
  year = 1965
}


[3]

@article{garcia2016modelo,
  title={Modelo de ciclos socio-tecnol{\'o}gicos para productos social y ambientalmente responsables. Caso: corte intensivo de rosas con energ{\'\i}a humana},
  author={Garc{\'\i}a Acosta, Gabriel},
  year={2016},
  publisher={Universitat Polit{\`e}cnica de Catalunya}
}


presentan retos en términos estadísticos. 

Dada una muestra aleatoria $X_i$ con distribución dada por una Función de Densidad Acuulada $F$, la Función Empírica de Densidad Acumulada se define de la siguiente manera:

\[\widehat{f}(x) = \frac{1}{n} \sum_{i = 1}^n I_{(-\infty, x]}(X_i)\]

Se trata de una función escalonada que aumenta en $\frac{1}{n}$ en cada uno de los puntos de la muestra. El teorema de Glivenko Cantelli (ver más adelante) garantiza que esta función converge puntualmente a $F$ con probabilidad igual a 1. En consecuencia la FEDA es un estimador de la Función de Densidad Acumulada asociada a $X$. El tamaño de los escalones es $\frac{1}{n}$, sin embargo es posible reemplazarlo por los pesos muestrales $a_i$ desde que todos sean positivos y su suma sea igual a 1.

