# Introducción

El desarrollo tecnológico de los últimos 20 años ha cambiado por completo los conceptos y procesos de análisis de la información. El aumento de la capacidad computacional conlleva la aparición de algoritmos y técnicas que, exigiendo mucho más sobre la máquina, permiten extraer información más precisa y útil. De manera simultánea estos desarrollos han conducido a la gestión y almacenamiento de grandes conjuntos de datos; conjuntos cuyo procesamiento y análisis presentan retos en términos estadísticos. Esto ha impulsado el estudio y desarrollo de diversas metodogías y herramientas analíticas, entre ellas se encuentra la Función de Densidad Acumulada Empírica, cuyo estudio es el eje central del presente trabajo.

Dada una muestra aleatoria $X_i$, la Función Empírica de Densidad Acumulada se define de la siguiente manera:

\[\overwide{f}(x) = \frac{1}{n} \sum_{i = 1}^n I_{(-\infty, x]}(X_i)\]

Se trata de una función escalonada que aumenta en $\frac{1}{n}$ en cada uno de los puntos de la muestra. El teorema de Glivenko Cantelli (ver más adelante) garantiza que esta función converge puntualmente con probabilidad 1 a la función de densidad acumulada de $X$. En consecuencia las función empírica de densidad acumulada es un estimador de la función de densidad acumulada asociada a $X$. El tamaño de los escalones es $\frac{1}{n}$, sin embargo es posible reemplazarlo por los pesos muestrales $a_i$ desde que todos sean positivos y su suma sea igual a 1.

En este contexto un conjunto de puntos es representado completamente por una curva. En consecuencia es posible representar una colección de conjuntos de puntos usando una colección de curvas. (el autor de epmeans) aplica algoritmos de agrupación a estas curvas, separándolas en un número dado de grupos de curvas similares entre sí. De esta manera es posible afirmar que EPMeans es un algoritmo de agrupación asintótica no paramétrico aplicable en grandes conjuntos de datos y tiene por objetivo encontrar una estructura de agrupación basada en la forma distribucional empírica dada por los datos. Este trabajo tiene aplicación en varios problemas estadísticos  que se mostrarán durante el desarrollo del estudio, uno de estos es la comparación de muestras de varias distribuciones.

@Aponte2012 aborda en su disertación de maestría el problema de comparación de muestras de varias distribuciones. Dado un número determinado de muestras, el problema consiste en encontrar diferencias distribucionales entre ellas. En respuesta a este problema existen pruebas estadísticas como ANOVA [@Montgomery2004], Kruskall Wallis [@KWTEST], Friedman [@Friedman1937], amén de un largo número de procedimientos de comparaciones mútiples [@CARSWA].

Las herramientas actuales hacen posible establecer cuándo una distribución presenta valores significativamente distintos a otra, sin embargo poseen inconvenientes en cuanto a interpretabilidad y visualización. La construcción de modelos estadísticos de tipo inferencial tiene en general supuestos distribucionales que en muchas ocasiones no permiten una conceptualización nítida del comportamiento global de los datos. Un resultado usual en este ambito es $\mu_i = \mu_j$, $\mu_j = \mu_k$ y $\mu_i \noteq \mu_k$. Esto sucede porque el error tipo I aumenta al aumentar el número de pruebas; en respuesta se amplían los intervalos de confianza llegando así a colnclusiones contraintuitivas.

Henderson y Clark (cita) distinguen dos componentes en la creación de conocimiento. El conocimiento modular se refiere a las partes del proceso y el conocimiento arquitectural se refiere a la estructura del mismo. Agrupar las poblaciones a partir de su comportamiento estocástico facilita la creación de perfiles, que muestren las tendencias principales con una visualización específica. Es posible decir que se trata de un avance arquitectural que nace en respuesta a las necesidades del investigador.

El presente trabajo recoge una fundamentación matemática al algoritmo EP-Means [@EPMEANS] introduciendo definiciones formales de las nubes de puntos, y de las funciones empíricas; y estudiando sus distancias y sus propiedades. Si bien la idea original del algoritmo se plantea a nivel univariado, la formalización permite generalizar sus conceptos al ámbito multivariado. Con esto se obtiene el soporte teórico necesario antes de entrar en el contexto aplicado.

**El algoritmo EP-Means hace uso de K-Means [@HARTIGAN] para agrupar las curvas. Una de las pre Implementar de el método de Ward [@WARD] y los algoritmos de agrupación ECSAGO [@Leon2010] y Chamaleon [@Karypis1999] y examinar l.**

**Con estas bases se obtiene una propuesta metodológica dirigida al problema de comparación no paramétrica de varias poblaciones, ya mencionado. Esta propuesta se evalua desde el punto de vista experimental mediante un estudio de Monte Carlo en dos etapas. La primera consiste en comparar el uso de distintas herramientas y así observar cuales son los parámetros que optimizan el desempeño de la propuesta. La segunda compara otros enfoques y muestra el desempeño de la propuesta en relación con otras técnicas estadísticas diseñadas con objetivos similares.**

**El estudio finaliza con la aplicación de la metodología propuesta en datos reales. Para este fin se dispone de los datos correspondientes a las pruebas Saber11, facilitados para la investigación por el Instituto Colombiano para la Evaluación de la Educación ICFES, y los datos “phytophthora”, de expresión genética presentes en el paquete “acde” de R [@Acosta2015].**

# Objetivos

## General

Analizar, fundamentar, generalizar e implementar el algoritmo EP-Means de agrupación estocástica.

## Específicos

- Formalizar matemáticamente la definición de nubes de puntos y estudiar sus propiedades (dimensión, orden, unión, suma y equivalencia).

- Analizar el algoritmo EP-Means a la luz de estas propiedades. Explicar la estructura estocástica de las nubes de puntos usando métodos para visualización y reducción de dimensiones (ACP) y agrupación (e.g. K-Means, Ward, Chamaleon, ECSAGO, entre otros). 

- Evaluar la propuesta resultante mediante un estudio de simulación comparando distintos enfoques.

- Implementar y aplicar la propuesta resultante en datos reales con miras a crear un paquete en R.

# Antecedentes

## Agrupación estocástica

El algoritmo EP-Means [@EPMEANS] es una nueva técnica que aplica el algoritmo K-Means sobre funciones empíricas de distribución acumulada. Para esto hace uso de la distancia Earth Mover's [@emd] entre estas funciones. El desarrollo desemboca en un algoritmo eficiente, empírico, no paramétrico y basado en distancias.

Una idea bastante parecida fue presentada por @Barrera2014, en esta propuesta se suavizan las distribuciones acumuladas estimadas e implementan herramientas diseñadas para datos funcionales. El artículo propone la implementación de un método de agrupamiento jerárquico para funciones de densidad considerándolas datos funcionales. Para la implementación se representan en forma discreta de las funciones de densidad de probabilidad, posteriormente se usa la distancia de Hellinger con el fin de medir las distancias entre todas las curvas, y a su vez, se construye una estructura de agrupamiento jerárquico.
La idea de agrupar los datos a partir de su distribución es planteada por @Applegate2011, que aplica K-Means sobre histogramas multivariadoss. No obstante el número de clases de los histogramas pueden influir sobre los agrupamientos. El uso de funciones empíricas resuelve esto, sin embargo en @EPMEANS se desarrolla únicamente el caso univariado, dejando el multivariado para trabajo futuro.

## Análisis de Componentes Principales

El Análisis de Componentes Principales es un método no supervisado que busca resumir un conjunto de variables en ejes factoriales mediante una transformación ortogonal. Debido a que estas transformaciones ortogonales son isomorfismos, generalmente el número de ejes factoriales posible corresponde al número de variables incluidas en el conjunto de interés.

Los ejes factoriales reúnen el total de la variabilidad del conjunto de datos de forma descendente, así el primero recoge la mayor varianza posible, el segundo reúne una variabilidad menor que el primero pero contiene la mayor varianza restante y así sucesivamente. Estos ejes son calculados bajo la restricción de ortogonalidad, obteniendo un conjunto de ejes ortogonales. La cantidad de ejes factoriales a conservar depende de la proporción de variabilidad que el investigador necesite recoger.

Este análisis tiene tres usos comunes en la literatura; los ejes factoriales recogen de manera descendente la varianza de los datos, por consiguiente es posible tomar un número reducido de ejes cubriendo un porcentaje importante de la información presente en los datos, disminuyendo la dimensión de los mismos. En este mismo sentido el uso de los ejes factoriales para interpretación gráfica de los datos proporciona visualizaciones óptimas e intuitivas. Por último, si las variables presentan correlaciones fuertes, es posible dar interpretación a los ejes factoriales y crear indicadores que midan aspectos específicos que no tienen una expresión explícita en la base de datos. [@Jolliffe2002]

### Método de clasificación: K-means

Como se dijo antes el algoritmo K-Means [@HARTIGAN] es teóricamente equivalente a la estimación vía EM de una mixtura compuesta por $K$ distribuciones normales homocedásticas.

En este sentido el algoritmo tiene dos partes. Un paso de estimación, que define el centroide de cada grupo como el promedio de los individuos pertenecientes al mismo y un paso de maximización, que reasigna los individuos al grupo más cercano usando su distancia a los distintos centroides. Estos dos pasos se repiten hasta lograr convergencia.

El uso de distancias puede verse afectado por distintas causas, entre ellos el factor de escala de las variables; en el sentido que las variables de mayor escala tienen un mayor peso en el cálculo de las distancias entre individuos. En consecuencia es necesario un tratamiento previo de normalización o estandarización de las mismas. La solución mostrada en Lebart [@LEBART] es tomar como insumo el resultado del ACP, que además de eliminar el ruido inherente a los datos produce ejes de escalas similares provenientes de las variables estandarizadas.

Asimismo es necesario atender detalles como criterios de parada para la convergencia, distancia a usar (euclidiana, manhattan, Crámer-von Mises), el algoritmo EP-Means hace uso de la distancia Earth mover's, sin embargo es necesario realizar pruebas con varias opciones. Una definición correcta de los valores iniciales puede facilitar la convergencia, Lebart propone como valores iniciales los centroides provenientes del método de Ward.

## Teorema de Glivenko Cantelli

El teorema de Glivenko Cantelli es el eje teórico del algoritmo. Establece la convergencia de la función empírica de densidad acumulada a la función de densidad acumulada teórica.

Sea $\{X\}_i, i = 1...m$ una muestra aleatoria con función de distribución acumulada $F$. La función de distribución empírica para el conjunto de v.a se define por:

\[F_n(x) = \frac{1}{n}\sum\limits_{i=1}^m I_{(-\infty, x]} (X_i) \]

Donde $I_A$ es la función indicadora en el conjunto $A$. Para cada $x$ fijo $F_n(x)$ es una secuencia de variables aleatorias las cuales convergen de manera casi segura a  $F(x)$.

El supuesto de independencia no es necesario para este teorema, y esto es fácilmente se observa que la misma conclusión se da cuando la secuencia de variables aleatorias es estrictamente una secuencia estacionaria y ergódica [@GCTHEO].

Con esto en mente agrupar las funciones de densidad empíricas por su cercanía puede mostrar los comportamientos asintóticos de las nubes de puntos.
